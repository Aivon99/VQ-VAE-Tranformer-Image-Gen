{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7fbbafb2",
      "metadata": {
        "id": "7fbbafb2"
      },
      "source": [
        "# Training Notebook for VQ-VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b52c75",
      "metadata": {
        "id": "e5b52c75"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strftime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-nf_hUh88R5",
      "metadata": {
        "id": "R-nf_hUh88R5"
      },
      "source": [
        "### Load CelebA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "URPH1QoEOcWg",
      "metadata": {
        "id": "URPH1QoEOcWg"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gWJUEtAeQhhZ",
      "metadata": {
        "id": "gWJUEtAeQhhZ"
      },
      "outputs": [],
      "source": [
        "# access the kaggle.json API key from the main folder of your google drive\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# download the dataset from kaggle\n",
        "!kaggle datasets download -d zuozhaorui/celeba\n",
        "!mkdir ./data\n",
        "!unzip -q celeba.zip -d ./data/celeba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKe80mkz498P",
      "metadata": {
        "id": "AKe80mkz498P"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "class CelebATransform:\n",
        "    '''\n",
        "    Crops around the face and resizes to 64x64. Output is a tensor of shape (3, 64, 64) scaled to [0, 1]\n",
        "    '''\n",
        "    def __call__(self, img):\n",
        "        img = torchvision.transforms.functional.crop(img, top=60, left=25, height=128, width=128)\n",
        "        img = torchvision.transforms.functional.resize(img, (64, 64))\n",
        "        img = torchvision.transforms.functional.to_tensor(img)\n",
        "        # img = torchvision.transforms.functional.normalize(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        return img\n",
        "celeba = torchvision.datasets.ImageFolder(root='./data/celeba', transform=CelebATransform())\n",
        "\n",
        "# visualize\n",
        "grid_x = 5\n",
        "grid_y = 4\n",
        "\n",
        "samples = torch.stack([celeba[i][0] for i in range(grid_x*grid_y)])\n",
        "\n",
        "img = torchvision.utils.make_grid(samples, grid_x, normalize=True, value_range=(0, 1))\n",
        "plt.title(f'Sample Images')\n",
        "plt.axis('off')\n",
        "plt.imshow(img.permute(1,2,0).cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4WpiSXGJ8193",
      "metadata": {
        "id": "4WpiSXGJ8193"
      },
      "source": [
        "### Define models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cbe8f8",
      "metadata": {
        "id": "14cbe8f8"
      },
      "outputs": [],
      "source": [
        "# clone the github repository containing the VQ-VAE model\n",
        "!git clone https://github.com/patrickmastorga/VQ-VAE-Tranformer-Image-Gen.git\n",
        "# import VQ-VAE model from model.py\n",
        "os.chdir('VQ-VAE-Tranformer-Image-Gen/VQ')\n",
        "from model import Encoder, Decoder, Quantizer, VQ_VAE, LATENT_DIM, EMBEDDING_DIM\n",
        "os.chdir('../../')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tr-m5WkYRlx7",
      "metadata": {
        "id": "Tr-m5WkYRlx7"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pg7sWQCJ3IZ5",
      "metadata": {
        "id": "pg7sWQCJ3IZ5"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "\n",
        "# initialize dataloader, models, and optimizer for training\n",
        "dataloader = torch.utils.data.DataLoader(celeba, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "quantizer = Quantizer(use_EMA=True, batch_size=BATCH_SIZE)\n",
        "model = VQ_VAE(encoder, decoder, quantizer, use_EMA=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Eps9W3v06jRc",
      "metadata": {
        "collapsed": true,
        "id": "Eps9W3v06jRc"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 0\n",
        "LOG_INTERVAL = 100\n",
        "SAVE_INTERVAL = 1000\n",
        "BETA = 0.1\n",
        "\n",
        "CHECKPOINT_DIR = '/content/drive/MyDrive/vq_models'\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pt')\n",
        "LOAD_FROM_CHECKPOINT = True\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "training_losses = []\n",
        "training_steps = 0\n",
        "running_losses = [0.0, 0.0, 0.0]\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# load from checkpoint\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f'WARNING: Checkpoint not found at {CHECKPOINT_PATH}!')\n",
        "    else:\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "        training_steps = checkpoint['training_steps']\n",
        "        training_losses = checkpoint['training_losses']\n",
        "        running_losses = checkpoint['running_losses']\n",
        "\n",
        "        print(f'Checkpoint loaded. Resuming from training step {training_steps}.')\n",
        "\n",
        "total_steps = training_steps + len(dataloader) * EPOCHS\n",
        "\n",
        "print(f'{strftime('%H:%M:%S')} Begin Training')\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in dataloader:\n",
        "        # training step\n",
        "        optimizer.zero_grad()\n",
        "        images, _ = batch\n",
        "        images = images.to(device)\n",
        "\n",
        "        reconstruction_loss, commitment_loss, codebook_loss = model(images)\n",
        "        if model.use_EMA:\n",
        "            loss = reconstruction_loss + BETA * commitment_loss + codebook_loss\n",
        "        else:\n",
        "            loss = reconstruction_loss + BETA * commitment_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_steps += 1\n",
        "\n",
        "        running_losses[0] += loss.item()\n",
        "        running_losses[1] += reconstruction_loss.item()\n",
        "        running_losses[2] += commitment_loss.item()\n",
        "\n",
        "        # keep track of loss and epoch progress\n",
        "        if training_steps % LOG_INTERVAL == 0:\n",
        "            avg_losses = [loss / LOG_INTERVAL for loss in running_losses]\n",
        "            running_losses = [0.0, 0.0, 0.0]\n",
        "            training_losses.append((training_steps, avg_losses))\n",
        "            with torch.no_grad():\n",
        "                p = model.quantizer.N / model.quantizer.N.sum() * 512\n",
        "                print(f'{strftime('%H:%M:%S')} TRAINING Step [{training_steps}/{total_steps}]; Loss: {avg_losses[0]:.4f}; Commitment: {avg_losses[2]:.4f}; Usage (min/med/max/dead): {p.min().item():.2f}, {p.median().item():.2f}, {p.max().item():.2f}, {torch.sum(p < 0.01) / 512 * 100:.0f}%')\n",
        "\n",
        "        if training_steps % SAVE_INTERVAL == 0:\n",
        "            checkpoint = {\n",
        "                'training_steps': training_steps,\n",
        "                'model_state': model.state_dict(),\n",
        "                'optimizer_state': optimizer.state_dict(),\n",
        "                'training_losses': training_losses,\n",
        "                'running_losses': running_losses,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, CHECKPOINT_PATH)\n",
        "            print(f'Checkpoint saved at step {training_steps} to {CHECKPOINT_PATH}')\n",
        "\n",
        "            # visualize reconstructions\n",
        "            samples, _ = next(iter(dataloader))\n",
        "            samples = samples[:5].to(device)\n",
        "\n",
        "            model.eval()\n",
        "            reconstructed = model.reconstruct(samples)\n",
        "            model.train()\n",
        "\n",
        "            img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), 5, normalize=True, value_range=(0, 1))\n",
        "            plt.title(f'Reconstructions at step {training_steps}')\n",
        "            plt.axis('off')\n",
        "            plt.imshow(img.permute(1,2,0).cpu())\n",
        "            plt.show()\n",
        "\n",
        "print(f'Training complete.')\n",
        "\n",
        "# Prepare loss data\n",
        "steps_list = [item[0] for item in training_losses]\n",
        "avg_losses_list = [item[1] for item in training_losses]\n",
        "\n",
        "steps = np.array(steps_list)\n",
        "losses = np.array(avg_losses_list)   # shape: (num_steps, 3)\n",
        "\n",
        "# Prepare reconstructions\n",
        "samples, _ = next(iter(dataloader))\n",
        "samples = samples[:10].to(device)\n",
        "\n",
        "model.eval()\n",
        "reconstructed = model.reconstruct(samples)\n",
        "model.train()\n",
        "\n",
        "img = torchvision.utils.make_grid(torch.cat((samples, reconstructed), dim=0), nrow=5, normalize=True, value_range=(0, 1))\n",
        "\n",
        "# Side-by-side plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(steps, losses[:, 0], label='Loss')\n",
        "axes[0].plot(steps, losses[:, 1], label='Reconstruction Loss')\n",
        "axes[0].plot(steps, losses[:, 2], label='Commitment Loss')\n",
        "\n",
        "axes[0].set_xlabel('Training Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Losses over Training Steps')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Reconstructions\n",
        "axes[1].imshow(img.permute(1, 2, 0).cpu())\n",
        "axes[1].set_title('Reconstructions')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D3O6D1xNNbeI",
      "metadata": {
        "id": "D3O6D1xNNbeI"
      },
      "source": [
        "### Compute CelebA embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x4cvj7nkNtVk",
      "metadata": {
        "id": "x4cvj7nkNtVk"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_DIR = '/content/drive/MyDrive/vq_models'\n",
        "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'checkpoint.pt')\n",
        "LOAD_FROM_CHECKPOINT = True\n",
        "\n",
        "# load from checkpoint\n",
        "if LOAD_FROM_CHECKPOINT:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        print(f'WARNING: Checkpoint not found at {CHECKPOINT_PATH}!')\n",
        "    else:\n",
        "        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state'])\n",
        "        print(f'Checkpoint loaded.')\n",
        "\n",
        "celeba_indices = []\n",
        "\n",
        "model.eval()\n",
        "for batch in tqdm(dataloader):\n",
        "    images, _ = batch\n",
        "    images = images.to(device)\n",
        "    indices = model.compute_indices(images)\n",
        "    celeba_indices.append(indices.view(images.shape[0], LATENT_DIM).cpu())\n",
        "\n",
        "print('Concatenating...')\n",
        "celeba_indices = torch.cat(celeba_indices, dim=0)\n",
        "print('Shape:', celeba_indices.shape)\n",
        "celeba_indices = celeba_indices.to(torch.uint16)\n",
        "print('Saving...')\n",
        "torch.save(celeba_indices, os.path.join(CHECKPOINT_DIR, \"celeba_vq_indices_uint16.pt\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
